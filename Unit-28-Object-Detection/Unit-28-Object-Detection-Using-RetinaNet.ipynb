{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Unit 28 - Project: Object Detection using RetinaNet\n",
    "\n",
    "**Project**: Object Detection, Classification and Labeling using RetinaNet\n",
    "\n",
    "\n",
    "## Course: Fall 2018, Deep Learning\n",
    "Professor: **Dr. James Shanahan**\n",
    "\n",
    "Students: **Gelesh Omathil and Murali Cheruvu**\n",
    "\n",
    "University: **Indiana University**\n",
    "\n",
    "\n",
    "**RetinaNet**: Introduction: https://arxiv.org/pdf/1708.02002.pdf\n",
    "\n",
    "**Dataset**: \n",
    "-\tUse **COCO Dataset** (http://cocodataset.org/#home) (~100k images) for training, validation and test datasets \n",
    "-\tAbout 1MM bounding boxes; some of the images have about 10 classes in them\n",
    "-\tWe have 80 classes in this database\n",
    "-\tOur focus is only 7 classes - **car, truck, person, bus, bycle and traffic sign**\n",
    "\n",
    "**Cloud**: Cloud Provider Server with Linux/Ubuntu Box with **GPU**s\n",
    "\n",
    "**Project**: **Train RetinaNet Dataset - Object Detectors**\n",
    "\n",
    "-\tUse this notebook as a base: https://github.com/fizyr/keras-retinanet\n",
    "-\tUse Transfer Learning (load the weights from pre-trained models)\n",
    "-\tUse base model on the pre-trained model from RetinaNet but focus on only 7 classes (all the other classes be treated like background images)\n",
    "-\tRetrain part of the network (about 6 key layers) from the transferred learning state using ResNet50/ResNet101 as a back-bone - with focus on 7 classes, so that we will recalibrate our model\n",
    "-\tPredict bounding boxes, predict classes - 8 classes (7 + background class), 8X5 outputs\n",
    "-\tTry out - output layers of different resolutions - ex: 56X56, 28x28, 14x14 (feature pyramid)\n",
    "-\tFor each feature pyramid, we will have output layer with loss function\n",
    "-\tTry out with smaller epochs with CPU and full blown using GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries\n",
    "\n",
    "- Python 3.6 \n",
    "- Keras 2.2.4+\n",
    "- TensorFlow (CPU and GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recognizing an object from an image has always been a very challenging task. If we need detect multiple objects from the same image, it is even more difficult. Purpose of Computer Vision is to solve such complex tasks. With the emergence of Neural Network driven Machine Learning algorithms, there are better ways to tackle these tasks. \n",
    "\n",
    "Object detection architecture is categorized into two types: two-stage and single-stage. \n",
    "\n",
    "Two-stage object detectors organize the image into two parts: foreground and background. Then, all the foreground objects are classified into more fine grained classes: car, truck, person, bus, bycle, etc. \n",
    "\n",
    "Convolutional Neural Network (CNN), Deep Learning, is an advanced neural network concept to perfectly handle these challenges.\n",
    "\n",
    "We present three techniques here - (1) Region-based CNN (R-CNN), (2)  Fast R-CNN and (3) Regional Proposal Network (RPN) (Ref: @Guide-DL).\n",
    "\n",
    "1. **Region-based CNN**\n",
    "\n",
    "![image.png](img/r-cnn.png)(Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "2. **Fast R-CNN**\n",
    "\n",
    "![image.png](img/faster-r-cnn.png) (Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "3. **Regional Proposal Network**\n",
    "\n",
    "![](img/reg_prop_1.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/reg_prop_2.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet - One-Stage Detector\n",
    "\n",
    "Most of the popular object detector algorithms are based on R-CNN with two-stage detection and give highest possible accuracy.\n",
    "However, two-stage detection algorithms are slower due to complex processing in a iterative manner. \n",
    "\n",
    "Recent work to improve the performance of the algorithms, one-stage detectors come to popularity. **OverFeat** and **YOLO** (You Only Look Once) have achieved faster detection with 10%-40% accuracy relative to two-stage detectors. \n",
    "\n",
    "RetinaNet, Focal Loss for Dense Object Detection, is a project done by Facebook AI Research team, has proposed one-stage detector with hybrid approaches from two-stage detectors such as Feature Pyramid Network (FPN) and Mask R-CNN, to achieve the accuracy comparable with two-stage detectors. RetinaNet offers best of both single-stage and two-stage detectors.\n",
    "\n",
    "Following figure shows the comparison of various object detection algorithms including RetinaNet:\n",
    "\n",
    "![image.png](img/retinanet-compare.png) (Ref: @ObjDetect)\n",
    "\n",
    "Some of the key aspects are listed as follows:\n",
    "\n",
    "- First-pass detection, class imbalance and inefficiency is addressed using techniques such as bootstrapping and hard example mining\n",
    "- Proposed a new loss function, **Focal Loss**, dynamically scaled cross entropy loss to deal with class imbalance using intuitive scaling factor to down-weight the contribution of easy samples automatically while focusing on the hard samples\n",
    "\n",
    "\n",
    "(Ref: @RetinaNet-Intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "# RetinaNet Components\n",
    "\n",
    "\"**RetinaNet is a single, unified network composed of a Feature Pyramid (backbone) network and two task-specific sub-networks**\" (Ref: @RetinaNet-Intro)\n",
    "\n",
    "![RetinaNet](img/retinanet.png \"Title\") (Ref: @RetinaNet-Intro)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet, CNN Network as Backbone\n",
    "\n",
    "ResNet-50 is a popular convolutional neural network for images. It processes images by going through several convolutional filters/kernels to create various feature-maps of the images to capture high level features, then it goes down into details with smaller feature maps by using pooling layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pyramid Network\n",
    "\n",
    "RetinaNet adds a Feature Pyramid Network (FPN), instead of, the typical classifier. Thus, RetinaNet collects feature maps at various layers from the ResNet and provides complex features at different scales. It is called pyramid network because it detects objects at different scales at different levels as it goes up in the pyramid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Boxes\n",
    "\n",
    "An anchor is a rectangle box with different sizes and ratios. At each FPN level, anchors are created in association with feature maps, covering each potential object. \n",
    "\n",
    "Each FPN level goes through two fully convolutional networks (FCN), first one is to find the regression - predicts anchor box boundaries - x1, y1, x2, y2 and the second neural network is for multi-label (N) classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss\n",
    "\n",
    "Real improvement in the accuracy of the RetinaNet is brought by using a new loss function called - Focal Loss.\n",
    "\n",
    "Focal Loss is designed to address the image imbalance challenge between foreground and background classes during the training of the image dataset. Focal Loss assigns low-weights to the well-defined backgrounds. \n",
    "\n",
    "Focal Loss for the binary classification, similar to Cross Entropy (CE):\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        CE_{(p,y)}=\\begin{cases}\n",
    "                -log(p) & \\text{if }y = 1\\,,  \\\\\n",
    "                -log(1 - p) & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "In the above y belongs to {+/- 1} denotes the base class (ground-truth) and p = [0,1] is the estimated probability of the model for the class with label y = 1. We define p as:\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        p_{t}=\\begin{cases}\n",
    "                p & \\text{if }y = 1\\,,  \\\\\n",
    "                1 - p & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        Fl(p_{t})= -(1 - p_{t})^{y} log(p_{t})\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the focal loss function defined above, classification cross-entropy loss -log(p) by a factor of (1-p)^y. Here is y is the modulating factor between 0 and 5. The well classified background classes have higher p and lower y. This is key aspect that compels the model to learn on specific foreground classes. \n",
    "\n",
    "\n",
    "\n",
    "![RetinaNet](img/focal-loss.png \"Focal Loss\") (Ref: @ObjDetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For complete details of the Focal Loss Object Detetion - Single-Stage Detector algorithm, please refer to the link:  https://arxiv.org/pdf/1708.02002.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- Prepare the dataset in the CSV format (with training and cross-validaton split)\n",
    "- Check the correctness of the dataset using retinanet-debug\n",
    "- Train retinanet, using predefined COCO weights (with decent jump start with better accuracy and better performance)\n",
    "- Optimize the training model to an inference model\n",
    "- Evaluate the updated model on the cross-validaton and test datasets\n",
    "- install pycocotools to test on the MS COCO dataset by running pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "- COCO dataset can be trained on RetinaNet using the python code lised in the training folder\n",
    "- The default backbone is ResNet50, it can be changed to a different dataset by pasing the dataset name in the --backbone argument\n",
    "- Various backbone models to try are: ResNet models (ResNet50, ResNet101), MobileNet models (MobileNet128_1:0, MobileNet128_0.75) and VGG models\n",
    "\n",
    "Trained model needs to converted into an inteference model before proceeding to the testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Usage\n",
    "\n",
    "### Running directly from the repository:\n",
    "keras_retinanet/bin/train.py coco /path/to/MS/COCO\n",
    "\n",
    "### Using the installed script:\n",
    "retinanet-train coco /path/to/MS/COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:14.892967Z",
     "start_time": "2018-12-12T01:11:12.034887Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:16.797828Z",
     "start_time": "2018-12-12T01:11:16.615817Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def read_image_bgr(path):\n",
    "    \"\"\" Read an image in BGR format.\n",
    "\n",
    "    Args\n",
    "        path: Path to the image.\n",
    "    \"\"\"\n",
    "    image = np.asarray(Image.open(path).convert('RGB'))\n",
    "    return image[:, :, ::-1].copy()\n",
    "\n",
    "def preprocess_image(x, mode='caffe'):\n",
    "    \"\"\" Preprocess an image by subtracting the ImageNet mean.\n",
    "\n",
    "    Args\n",
    "        x: np.array of shape (None, None, 3) or (3, None, None).\n",
    "        mode: One of \"caffe\" or \"tf\".\n",
    "            - caffe: will zero-center each color channel with\n",
    "                respect to the ImageNet dataset, without scaling.\n",
    "            - tf: will scale pixels between -1 and 1, sample-wise.\n",
    "\n",
    "    Returns\n",
    "        The input with the ImageNet mean subtracted.\n",
    "    \"\"\"\n",
    "    # mostly identical to \"https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py\"\n",
    "    # except for converting RGB -> BGR since we assume BGR already\n",
    "\n",
    "    # covert always to float32 to keep compatibility with opencv\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    if mode == 'tf':\n",
    "        x /= 127.5\n",
    "        x -= 1.\n",
    "    elif mode == 'caffe':\n",
    "        x[..., 0] -= 103.939\n",
    "        x[..., 1] -= 116.779\n",
    "        x[..., 2] -= 123.68\n",
    "\n",
    "    return x\n",
    "\n",
    "def resize_image(img, min_side=800, max_side=1333):\n",
    "    \"\"\" Resize an image such that the size is constrained to min_side and max_side.\n",
    "\n",
    "    Args\n",
    "        min_side: The image's min side will be equal to min_side after resizing.\n",
    "        max_side: If after resizing the image's max side is above max_side, resize until the max side is equal to max_side.\n",
    "\n",
    "    Returns\n",
    "        A resized image.\n",
    "    \"\"\"\n",
    "    # compute scale to resize the image\n",
    "    scale = compute_resize_scale(img.shape, min_side=min_side, max_side=max_side)\n",
    "\n",
    "    # resize the image with the computed scale\n",
    "    img = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "\n",
    "    return img, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:17.787980Z",
     "start_time": "2018-12-12T01:11:17.528202Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_color(label):\n",
    "    \"\"\" Return a color from a set of predefined colors. Contains 80 colors in total.\n",
    "\n",
    "    Args\n",
    "        label: The label to get the color for.\n",
    "\n",
    "    Returns\n",
    "        A list of three values representing a RGB color.\n",
    "\n",
    "        If no color is defined for a certain label, the color green is returned and a warning is printed.\n",
    "    \"\"\"\n",
    "    if label < len(colors):\n",
    "        return colors[label]\n",
    "    else:\n",
    "        warnings.warn('Label {} has no color, returning default.'.format(label))\n",
    "        return (0, 255, 0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generated using:\n",
    "\n",
    "```\n",
    "colors = [list((matplotlib.colors.hsv_to_rgb([x, 1.0, 1.0]) * 255).astype(int)) for x in np.arange(0, 1, 1.0 / 80)]\n",
    "shuffle(colors)\n",
    "pprint(colors)\n",
    "```\n",
    "\"\"\"\n",
    "colors = [\n",
    "    [31  , 0   , 255] ,\n",
    "    [0   , 159 , 255] ,\n",
    "    [255 , 95  , 0]   ,\n",
    "    [255 , 19  , 0]   ,\n",
    "    [255 , 0   , 0]   ,\n",
    "    [255 , 38  , 0]   ,\n",
    "    [0   , 255 , 25]  ,\n",
    "    [255 , 0   , 133] ,\n",
    "    [255 , 172 , 0]   ,\n",
    "    [108 , 0   , 255] ,\n",
    "    [0   , 82  , 255] ,\n",
    "    [0   , 255 , 6]   ,\n",
    "    [255 , 0   , 152] ,\n",
    "    [223 , 0   , 255] ,\n",
    "    [12  , 0   , 255] ,\n",
    "    [0   , 255 , 178] ,\n",
    "    [108 , 255 , 0]   ,\n",
    "    [184 , 0   , 255] ,\n",
    "    [255 , 0   , 76]  ,\n",
    "    [146 , 255 , 0]   ,\n",
    "    [51  , 0   , 255] ,\n",
    "    [0   , 197 , 255] ,\n",
    "    [255 , 248 , 0]   ,\n",
    "    [255 , 0   , 19]  ,\n",
    "    [255 , 0   , 38]  ,\n",
    "    [89  , 255 , 0]   ,\n",
    "    [127 , 255 , 0]   ,\n",
    "    [255 , 153 , 0]   ,\n",
    "    [0   , 255 , 255] ,\n",
    "    [0   , 255 , 216] ,\n",
    "    [0   , 255 , 121] ,\n",
    "    [255 , 0   , 248] ,\n",
    "    [70  , 0   , 255] ,\n",
    "    [0   , 255 , 159] ,\n",
    "    [0   , 216 , 255] ,\n",
    "    [0   , 6   , 255] ,\n",
    "    [0   , 63  , 255] ,\n",
    "    [31  , 255 , 0]   ,\n",
    "    [255 , 57  , 0]   ,\n",
    "    [255 , 0   , 210] ,\n",
    "    [0   , 255 , 102] ,\n",
    "    [242 , 255 , 0]   ,\n",
    "    [255 , 191 , 0]   ,\n",
    "    [0   , 255 , 63]  ,\n",
    "    [255 , 0   , 95]  ,\n",
    "    [146 , 0   , 255] ,\n",
    "    [184 , 255 , 0]   ,\n",
    "    [255 , 114 , 0]   ,\n",
    "    [0   , 255 , 235] ,\n",
    "    [255 , 229 , 0]   ,\n",
    "    [0   , 178 , 255] ,\n",
    "    [255 , 0   , 114] ,\n",
    "    [255 , 0   , 57]  ,\n",
    "    [0   , 140 , 255] ,\n",
    "    [0   , 121 , 255] ,\n",
    "    [12  , 255 , 0]   ,\n",
    "    [255 , 210 , 0]   ,\n",
    "    [0   , 255 , 44]  ,\n",
    "    [165 , 255 , 0]   ,\n",
    "    [0   , 25  , 255] ,\n",
    "    [0   , 255 , 140] ,\n",
    "    [0   , 101 , 255] ,\n",
    "    [0   , 255 , 82]  ,\n",
    "    [223 , 255 , 0]   ,\n",
    "    [242 , 0   , 255] ,\n",
    "    [89  , 0   , 255] ,\n",
    "    [165 , 0   , 255] ,\n",
    "    [70  , 255 , 0]   ,\n",
    "    [255 , 0   , 172] ,\n",
    "    [255 , 76  , 0]   ,\n",
    "    [203 , 255 , 0]   ,\n",
    "    [204 , 0   , 255] ,\n",
    "    [255 , 0   , 229] ,\n",
    "    [255 , 133 , 0]   ,\n",
    "    [127 , 0   , 255] ,\n",
    "    [0   , 235 , 255] ,\n",
    "    [0   , 255 , 197] ,\n",
    "    [255 , 0   , 191] ,\n",
    "    [0   , 44  , 255] ,\n",
    "    [50  , 255 , 0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:18.812435Z",
     "start_time": "2018-12-12T01:11:18.634065Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_box(image, box, color, thickness=2):\n",
    "    \"\"\" Draws a box on an image with a given color.\n",
    "\n",
    "    # Arguments\n",
    "        image     : The image to draw on.\n",
    "        box       : A list of 4 elements (x1, y1, x2, y2).\n",
    "        color     : The color of the box.\n",
    "        thickness : The thickness of the lines to draw a box with.\n",
    "    \"\"\"\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.rectangle(image, (b[0], b[1]), (b[2], b[3]), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def draw_caption(image, box, caption):\n",
    "    \"\"\" Draws a caption above the box in an image.\n",
    "\n",
    "    # Arguments\n",
    "        image   : The image to draw on.\n",
    "        box     : A list of 4 elements (x1, y1, x2, y2).\n",
    "        caption : String containing the text to draw.\n",
    "    \"\"\"\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:19.575336Z",
     "start_time": "2018-12-12T01:11:19.391490Z"
    }
   },
   "outputs": [],
   "source": [
    "class PriorProbability(keras.initializers.Initializer):\n",
    "    \"\"\" Apply a prior probability to the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probability=0.01):\n",
    "        self.probability = probability\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'probability': self.probability\n",
    "        }\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # set bias to -log((1 - p)/p) for foreground\n",
    "        result = np.ones(shape, dtype=dtype) * -math.log((1 - self.probability) / self.probability)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T01:11:21.006215Z",
     "start_time": "2018-12-12T01:11:20.817707Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (compute_overlap.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"D:\\Murali\\DataScience\\E533-DeepLearning\\my_course\\assignments\\Unit-28-Object-Detection\\compute_overlap.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    cimport cython\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%run filter_detections.py \n",
    "%run tensorflow_backend.py \n",
    "%run anchors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T00:42:19.614254Z",
     "start_time": "2018-12-12T00:42:19.412740Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-f08b6b4bd9f7>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-f08b6b4bd9f7>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    from ./keras_retinanet/ import layers\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright 2017-2018 Fizyr (https://fizyr.com)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "\n",
    "def default_classification_model(\n",
    "    num_classes,\n",
    "    num_anchors,\n",
    "    pyramid_feature_size=256,\n",
    "    prior_probability=0.01,\n",
    "    classification_feature_size=256,\n",
    "    name='classification_submodel'\n",
    "):\n",
    "    \"\"\" Creates the default regression submodel.\n",
    "\n",
    "    Args\n",
    "        num_classes                 : Number of classes to predict a score for at each feature level.\n",
    "        num_anchors                 : Number of anchors to predict classification scores for at each feature level.\n",
    "        pyramid_feature_size        : The number of filters to expect from the feature pyramid levels.\n",
    "        classification_feature_size : The number of filters to use in the layers in the classification submodel.\n",
    "        name                        : The name of the submodel.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model that predicts classes for each anchor.\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        'kernel_size' : 3,\n",
    "        'strides'     : 1,\n",
    "        'padding'     : 'same',\n",
    "    }\n",
    "\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        inputs  = keras.layers.Input(shape=(pyramid_feature_size, None, None))\n",
    "    else:\n",
    "        inputs  = keras.layers.Input(shape=(None, None, pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "    for i in range(4):\n",
    "        outputs = keras.layers.Conv2D(\n",
    "            filters=classification_feature_size,\n",
    "            activation='relu',\n",
    "            name='pyramid_classification_{}'.format(i),\n",
    "            kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "            bias_initializer='zeros',\n",
    "            **options\n",
    "        )(outputs)\n",
    "\n",
    "    outputs = keras.layers.Conv2D(\n",
    "        filters=num_classes * num_anchors,\n",
    "        kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        bias_initializer=initializers.PriorProbability(probability=prior_probability),\n",
    "        name='pyramid_classification',\n",
    "        **options\n",
    "    )(outputs)\n",
    "\n",
    "    # reshape output and apply sigmoid\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        outputs = keras.layers.Permute((2, 3, 1), name='pyramid_classification_permute')(outputs)\n",
    "    outputs = keras.layers.Reshape((-1, num_classes), name='pyramid_classification_reshape')(outputs)\n",
    "    outputs = keras.layers.Activation('sigmoid', name='pyramid_classification_sigmoid')(outputs)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def default_regression_model(num_values, num_anchors, pyramid_feature_size=256, regression_feature_size=256, name='regression_submodel'):\n",
    "    \"\"\" Creates the default regression submodel.\n",
    "\n",
    "    Args\n",
    "        num_values              : Number of values to regress.\n",
    "        num_anchors             : Number of anchors to regress for each feature level.\n",
    "        pyramid_feature_size    : The number of filters to expect from the feature pyramid levels.\n",
    "        regression_feature_size : The number of filters to use in the layers in the regression submodel.\n",
    "        name                    : The name of the submodel.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model that predicts regression values for each anchor.\n",
    "    \"\"\"\n",
    "    # All new conv layers except the final one in the\n",
    "    # RetinaNet (classification) subnets are initialized\n",
    "    # with bias b = 0 and a Gaussian weight fill with stddev = 0.01.\n",
    "    options = {\n",
    "        'kernel_size'        : 3,\n",
    "        'strides'            : 1,\n",
    "        'padding'            : 'same',\n",
    "        'kernel_initializer' : keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer'   : 'zeros'\n",
    "    }\n",
    "\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        inputs  = keras.layers.Input(shape=(pyramid_feature_size, None, None))\n",
    "    else:\n",
    "        inputs  = keras.layers.Input(shape=(None, None, pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "    for i in range(4):\n",
    "        outputs = keras.layers.Conv2D(\n",
    "            filters=regression_feature_size,\n",
    "            activation='relu',\n",
    "            name='pyramid_regression_{}'.format(i),\n",
    "            **options\n",
    "        )(outputs)\n",
    "\n",
    "    outputs = keras.layers.Conv2D(num_anchors * num_values, name='pyramid_regression', **options)(outputs)\n",
    "    if keras.backend.image_data_format() == 'channels_first':\n",
    "        outputs = keras.layers.Permute((2, 3, 1), name='pyramid_regression_permute')(outputs)\n",
    "    outputs = keras.layers.Reshape((-1, num_values), name='pyramid_regression_reshape')(outputs)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def __create_pyramid_features(C3, C4, C5, feature_size=256):\n",
    "    \"\"\" Creates the FPN layers on top of the backbone features.\n",
    "\n",
    "    Args\n",
    "        C3           : Feature stage C3 from the backbone.\n",
    "        C4           : Feature stage C4 from the backbone.\n",
    "        C5           : Feature stage C5 from the backbone.\n",
    "        feature_size : The feature size to use for the resulting feature levels.\n",
    "\n",
    "    Returns\n",
    "        A list of feature levels [P3, P4, P5, P6, P7].\n",
    "    \"\"\"\n",
    "    # upsample C5 to get P5 from the FPN paper\n",
    "    P5           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C5_reduced')(C5)\n",
    "    P5_upsampled = layers.UpsampleLike(name='P5_upsampled')([P5, C4])\n",
    "    P5           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P5')(P5)\n",
    "\n",
    "    # add P5 elementwise to C4\n",
    "    P4           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C4_reduced')(C4)\n",
    "    P4           = keras.layers.Add(name='P4_merged')([P5_upsampled, P4])\n",
    "    P4_upsampled = layers.UpsampleLike(name='P4_upsampled')([P4, C3])\n",
    "    P4           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P4')(P4)\n",
    "\n",
    "    # add P4 elementwise to C3\n",
    "    P3 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C3_reduced')(C3)\n",
    "    P3 = keras.layers.Add(name='P3_merged')([P4_upsampled, P3])\n",
    "    P3 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P3')(P3)\n",
    "\n",
    "    # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "    P6 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P6')(C5)\n",
    "\n",
    "    # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "    P7 = keras.layers.Activation('relu', name='C6_relu')(P6)\n",
    "    P7 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P7')(P7)\n",
    "\n",
    "    return [P3, P4, P5, P6, P7]\n",
    "\n",
    "\n",
    "def default_submodels(num_classes, num_anchors):\n",
    "    \"\"\" Create a list of default submodels used for object detection.\n",
    "\n",
    "    The default submodels contains a regression submodel and a classification submodel.\n",
    "\n",
    "    Args\n",
    "        num_classes : Number of classes to use.\n",
    "        num_anchors : Number of base anchors.\n",
    "\n",
    "    Returns\n",
    "        A list of tuple, where the first element is the name of the submodel and the second element is the submodel itself.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ('regression', default_regression_model(4, num_anchors)),\n",
    "        ('classification', default_classification_model(num_classes, num_anchors))\n",
    "    ]\n",
    "\n",
    "\n",
    "def __build_model_pyramid(name, model, features):\n",
    "    \"\"\" Applies a single submodel to each FPN level.\n",
    "\n",
    "    Args\n",
    "        name     : Name of the submodel.\n",
    "        model    : The submodel to evaluate.\n",
    "        features : The FPN features.\n",
    "\n",
    "    Returns\n",
    "        A tensor containing the response from the submodel on the FPN features.\n",
    "    \"\"\"\n",
    "    return keras.layers.Concatenate(axis=1, name=name)([model(f) for f in features])\n",
    "\n",
    "\n",
    "def __build_pyramid(models, features):\n",
    "    \"\"\" Applies all submodels to each FPN level.\n",
    "\n",
    "    Args\n",
    "        models   : List of sumodels to run on each pyramid level (by default only regression, classifcation).\n",
    "        features : The FPN features.\n",
    "\n",
    "    Returns\n",
    "        A list of tensors, one for each submodel.\n",
    "    \"\"\"\n",
    "    return [__build_model_pyramid(n, m, features) for n, m in models]\n",
    "\n",
    "\n",
    "def __build_anchors(anchor_parameters, features):\n",
    "    \"\"\" Builds anchors for the shape of the features from FPN.\n",
    "\n",
    "    Args\n",
    "        anchor_parameters : Parameteres that determine how anchors are generated.\n",
    "        features          : The FPN features.\n",
    "\n",
    "    Returns\n",
    "        A tensor containing the anchors for the FPN features.\n",
    "\n",
    "        The shape is:\n",
    "        ```\n",
    "        (batch_size, num_anchors, 4)\n",
    "        ```\n",
    "    \"\"\"\n",
    "    anchors = [\n",
    "        layers.Anchors(\n",
    "            size=anchor_parameters.sizes[i],\n",
    "            stride=anchor_parameters.strides[i],\n",
    "            ratios=anchor_parameters.ratios,\n",
    "            scales=anchor_parameters.scales,\n",
    "            name='anchors_{}'.format(i)\n",
    "        )(f) for i, f in enumerate(features)\n",
    "    ]\n",
    "\n",
    "    return keras.layers.Concatenate(axis=1, name='anchors')(anchors)\n",
    "\n",
    "\n",
    "def retinanet(\n",
    "    inputs,\n",
    "    backbone_layers,\n",
    "    num_classes,\n",
    "    num_anchors             = None,\n",
    "    create_pyramid_features = __create_pyramid_features,\n",
    "    submodels               = None,\n",
    "    name                    = 'retinanet'\n",
    "):\n",
    "    \"\"\" Construct a RetinaNet model on top of a backbone.\n",
    "\n",
    "    This model is the minimum model necessary for training (with the unfortunate exception of anchors as output).\n",
    "\n",
    "    Args\n",
    "        inputs                  : keras.layers.Input (or list of) for the input to the model.\n",
    "        num_classes             : Number of classes to classify.\n",
    "        num_anchors             : Number of base anchors.\n",
    "        create_pyramid_features : Functor for creating pyramid features given the features C3, C4, C5 from the backbone.\n",
    "        submodels               : Submodels to run on each feature map (default is regression and classification submodels).\n",
    "        name                    : Name of the model.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model which takes an image as input and outputs generated anchors and the result from each submodel on every pyramid level.\n",
    "\n",
    "        The order of the outputs is as defined in submodels:\n",
    "        ```\n",
    "        [\n",
    "            regression, classification, other[0], other[1], ...\n",
    "        ]\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    if num_anchors is None:\n",
    "        num_anchors = AnchorParameters.default.num_anchors()\n",
    "\n",
    "    if submodels is None:\n",
    "        submodels = default_submodels(num_classes, num_anchors)\n",
    "\n",
    "    C3, C4, C5 = backbone_layers\n",
    "\n",
    "    # compute pyramid features as per https://arxiv.org/abs/1708.02002\n",
    "    features = create_pyramid_features(C3, C4, C5)\n",
    "\n",
    "    # for all pyramid levels, run available submodels\n",
    "    pyramids = __build_pyramid(submodels, features)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=pyramids, name=name)\n",
    "\n",
    "\n",
    "def retinanet_bbox(\n",
    "    model                 = None,\n",
    "    nms                   = True,\n",
    "    class_specific_filter = True,\n",
    "    name                  = 'retinanet-bbox',\n",
    "    anchor_params         = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\" Construct a RetinaNet model on top of a backbone and adds convenience functions to output boxes directly.\n",
    "\n",
    "    This model uses the minimum retinanet model and appends a few layers to compute boxes within the graph.\n",
    "    These layers include applying the regression values to the anchors and performing NMS.\n",
    "\n",
    "    Args\n",
    "        model                 : RetinaNet model to append bbox layers to. If None, it will create a RetinaNet model using **kwargs.\n",
    "        nms                   : Whether to use non-maximum suppression for the filtering step.\n",
    "        class_specific_filter : Whether to use class specific filtering or filter for the best scoring class only.\n",
    "        name                  : Name of the model.\n",
    "        anchor_params         : Struct containing anchor parameters. If None, default values are used.\n",
    "        *kwargs               : Additional kwargs to pass to the minimal retinanet model.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model which takes an image as input and outputs the detections on the image.\n",
    "\n",
    "        The order is defined as follows:\n",
    "        ```\n",
    "        [\n",
    "            boxes, scores, labels, other[0], other[1], ...\n",
    "        ]\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    # if no anchor parameters are passed, use default values\n",
    "    if anchor_params is None:\n",
    "        anchor_params = AnchorParameters.default\n",
    "\n",
    "    # create RetinaNet model\n",
    "    if model is None:\n",
    "        model = retinanet(num_anchors=anchor_params.num_anchors(), **kwargs)\n",
    "    else:\n",
    "        assert_training_model(model)\n",
    "\n",
    "    # compute the anchors\n",
    "    features = [model.get_layer(p_name).output for p_name in ['P3', 'P4', 'P5', 'P6', 'P7']]\n",
    "    anchors  = __build_anchors(anchor_params, features)\n",
    "\n",
    "    # we expect the anchors, regression and classification values as first output\n",
    "    regression     = model.outputs[0]\n",
    "    classification = model.outputs[1]\n",
    "\n",
    "    # \"other\" can be any additional output from custom submodels, by default this will be []\n",
    "    other = model.outputs[2:]\n",
    "\n",
    "    # apply predicted regression to anchors\n",
    "    boxes = layers.RegressBoxes(name='boxes')([anchors, regression])\n",
    "    boxes = layers.ClipBoxes(name='clipped_boxes')([model.inputs[0], boxes])\n",
    "\n",
    "    # filter detections (apply NMS / score threshold / select top-k)\n",
    "    detections = layers.FilterDetections(\n",
    "        nms                   = nms,\n",
    "        class_specific_filter = class_specific_filter,\n",
    "        name                  = 'filtered_detections'\n",
    "    )([boxes, classification] + other)\n",
    "\n",
    "    # construct the model\n",
    "    return keras.models.Model(inputs=model.inputs, outputs=detections, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Geneic Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T23:53:53.227473Z",
     "start_time": "2018-12-11T23:53:53.043742Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "\n",
    "class Backbone(object):\n",
    "    \"\"\" This class stores additional information on backbones.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone):\n",
    "        # a dictionary mapping custom layer names to the correct classes\n",
    "        from .. import layers\n",
    "        from .. import losses\n",
    "        from .. import initializers\n",
    "        self.custom_objects = {\n",
    "            'UpsampleLike'     : layers.UpsampleLike,\n",
    "            'PriorProbability' : initializers.PriorProbability,\n",
    "            'RegressBoxes'     : layers.RegressBoxes,\n",
    "            'FilterDetections' : layers.FilterDetections,\n",
    "            'Anchors'          : layers.Anchors,\n",
    "            'ClipBoxes'        : layers.ClipBoxes,\n",
    "            '_smooth_l1'       : losses.smooth_l1(),\n",
    "            '_focal'           : losses.focal(),\n",
    "        }\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.validate()\n",
    "\n",
    "    def retinanet(self, *args, **kwargs):\n",
    "        \"\"\" Returns a retinanet model using the correct backbone.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('retinanet method not implemented.')\n",
    "\n",
    "    def download_imagenet(self):\n",
    "        \"\"\" Downloads ImageNet weights and returns path to weights file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('download_imagenet method not implemented.')\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\" Checks whether the backbone string is correct.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('validate method not implemented.')\n",
    "\n",
    "    def preprocess_image(self, inputs):\n",
    "        \"\"\" Takes as input an image and prepares it for being passed through the network.\n",
    "        Having this function in Backbone allows other backbones to define a specific preprocessing step.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('preprocess_image method not implemented.')\n",
    "\n",
    "\n",
    "def backbone(backbone_name):\n",
    "    \"\"\" Returns a backbone object for the given backbone.\n",
    "    \"\"\"\n",
    "    if 'resnet' in backbone_name:\n",
    "        from .resnet import ResNetBackbone as b\n",
    "    elif 'mobilenet' in backbone_name:\n",
    "        from .mobilenet import MobileNetBackbone as b\n",
    "    elif 'vgg' in backbone_name:\n",
    "        from .vgg import VGGBackbone as b\n",
    "    elif 'densenet' in backbone_name:\n",
    "        from .densenet import DenseNetBackbone as b\n",
    "    else:\n",
    "        raise NotImplementedError('Backbone class for  \\'{}\\' not implemented.'.format(backbone))\n",
    "\n",
    "    return b(backbone_name)\n",
    "\n",
    "\n",
    "def load_model(filepath, backbone_name='resnet50'):\n",
    "    \"\"\" Loads a retinanet model using the correct custom objects.\n",
    "\n",
    "    Args\n",
    "        filepath: one of the following:\n",
    "            - string, path to the saved model, or\n",
    "            - h5py.File object from which to load the model\n",
    "        backbone_name         : Backbone with which the model was trained.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model object.\n",
    "\n",
    "    Raises\n",
    "        ImportError: if h5py is not available.\n",
    "        ValueError: In case of an invalid savefile.\n",
    "    \"\"\"\n",
    "    import keras.models\n",
    "    return keras.models.load_model(filepath, custom_objects=backbone(backbone_name).custom_objects)\n",
    "\n",
    "\n",
    "def convert_model(model, nms=True, class_specific_filter=True, anchor_params=None):\n",
    "    \"\"\" Converts a training model to an inference model.\n",
    "\n",
    "    Args\n",
    "        model                 : A retinanet training model.\n",
    "        nms                   : Boolean, whether to add NMS filtering to the converted model.\n",
    "        class_specific_filter : Whether to use class specific filtering or filter for the best scoring class only.\n",
    "        anchor_params         : Anchor parameters object. If omitted, default values are used.\n",
    "\n",
    "    Returns\n",
    "        A keras.models.Model object.\n",
    "\n",
    "    Raises\n",
    "        ImportError: if h5py is not available.\n",
    "        ValueError: In case of an invalid savefile.\n",
    "    \"\"\"\n",
    "    from .retinanet import retinanet_bbox\n",
    "    return retinanet_bbox(model=model, nms=nms, class_specific_filter=class_specific_filter, anchor_params=anchor_params)\n",
    "\n",
    "\n",
    "def assert_training_model(model):\n",
    "    \"\"\" Assert that the model is a training model.\n",
    "    \"\"\"\n",
    "    assert(all(output in model.output_names for output in ['regression', 'classification'])), \\\n",
    "        \"Input is not a training model (no 'regression' and 'classification' outputs were found, outputs are: {}).\".format(model.output_names)\n",
    "\n",
    "\n",
    "def check_training_model(model):\n",
    "    \"\"\" Check that model is a training model and exit otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert_training_model(model)\n",
    "    except AssertionError as e:\n",
    "        print(e, file=sys.stderr)\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RetinaNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T20:36:45.250681Z",
     "start_time": "2018-12-08T20:36:45.055623Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_resnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6f57b673b695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# load retinanet model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackbone_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'resnet50'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# if the model is not converted to an inference model, use the line below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Murali\\DataScience\\E533-Deep Learning\\my_course\\assignments\\Unit-28-Object-Detection\\keras_retinanet\\models\\__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, backbone_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \"\"\"\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackbone_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Murali\\DataScience\\E533-Deep Learning\\my_course\\assignments\\Unit-28-Object-Detection\\keras_retinanet\\models\\__init__.py\u001b[0m in \u001b[0;36mbackbone\u001b[1;34m(backbone_name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'resnet'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackbone_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mResNetBackbone\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m'mobilenet'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackbone_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMobileNetBackbone\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Murali\\DataScience\\E533-Deep Learning\\my_course\\assignments\\Unit-28-Object-Detection\\keras_retinanet\\models\\resnet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_resnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_resnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_resnet'"
     ]
    }
   ],
   "source": [
    "# adjust this to point to your downloaded/trained model\n",
    "# models can be downloaded here: https://github.com/fizyr/keras-retinanet/releases\n",
    "model_path = os.path.join('..', 'snapshots', 'resnet50_coco_best_v2.1.0.h5')\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# if the model is not converted to an inference model, use the line below\n",
    "# see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model\n",
    "#model = models.convert_model(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'person',  1: 'bicycle',  2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', \n",
    "                   6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', \n",
    "                   11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', \n",
    "                   16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', \n",
    "                   21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', \n",
    "                   26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', \n",
    "                   31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', \n",
    "                   36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', \n",
    "                   41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', \n",
    "                   46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', \n",
    "                   51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', \n",
    "                   56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', \n",
    "                   61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', \n",
    "                   66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', \n",
    "                   71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', \n",
    "                   76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_img_retinaNet(img_name):\n",
    "    # load image\n",
    "    image = read_image_bgr(img_name)\n",
    "\n",
    "    # copy to draw on\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    print(\"processing time: \", time.time() - start)\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < 0.5:\n",
    "            break\n",
    "\n",
    "        color = label_color(label)\n",
    "\n",
    "        b = box.astype(int)\n",
    "        draw_box(draw, b, color=color)\n",
    "\n",
    "        caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "        draw_caption(draw, b, caption)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#car, truck, person, bus, bycle and traffic sign\n",
    "classify_img_retinaNet('img_with_car.jpg')\n",
    "classify_img_retinaNet('img_with_truck.jpg')\n",
    "classify_img_retinaNet('img_with_person.jpg')\n",
    "classify_img_retinaNet('img_with_bus.jpg')\n",
    "classify_img_retinaNet('img_with_bycle.jpg')\n",
    "classify_img_retinaNet('img_with_traffic_sign.jpg')\n",
    "classify_img_retinaNet('img_with_multiple.jpg')\n",
    "classify_img_retinaNet('img_with_all.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Next Steps - Projects\n",
    "\n",
    "- NATO Innovation Challenge. The winning team of the NATO Innovation Challenge used keras-retinanet to detect cars in aerial images (COWC dataset).\n",
    "\n",
    "- Microsoft Research for Horovod on Azure. A research project by Microsoft, using keras-retinanet to distribute training over multiple GPUs using Horovod on Azure.\n",
    "\n",
    "- 4k video example. This demo shows the use of keras-retinanet on a 4k input video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# References: \n",
    "\n",
    "We would like thank our professor - **Dr. James Shanahan** for his great guidance, continual help and support during the **Deep Learning course.**\n",
    "\n",
    "We would also like to thank various developers and authors of the Deep Learning (CNN) related including the references given in the following links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Books **\n",
    "\n",
    "- Ref: Book_DL\n",
    "- Book Title: **Deep Learning**\n",
    "- Authors: Ian Goodfellow, Yoshua Bengio and Aaron Courville\n",
    "\n",
    "\n",
    "- Ref: Guide-DL\n",
    "- Book/Guide: **A Guide to Covolutional Neural Networks for Computer Vision**\n",
    "- Link: https://www.dropbox.com/s/789qiaq0svh4270/A%20Guide%20to%20Convolutional%20Neural%20Networks%20for%20Computer%20Vision.pdf?dl=0\n",
    "- Editors: Grard Medioni, University of Southern California and Sven Dickinson, University of Toronto\n",
    "\n",
    "** Videos **\n",
    "\n",
    "- Title: **Courseera CNN course - Object Detection and Localization**\n",
    "- Link: https://www.coursera.org/lecture/convolutional-neural-networks/object-detection-VgyWR\n",
    "- Professor: Andrew Ng\n",
    "\n",
    "\n",
    "** Web Articles **\n",
    "\n",
    "- Title: **Back-Propogation is very simple. Who made it complicated?**\n",
    "- Link: https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "- Author: Prakash Jay\n",
    "- Date: 20-Apr-2017\n",
    "\n",
    "\n",
    "- Title: **An intutive guide to Convolutional Neural Networks**\n",
    "- Link: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\n",
    "- Author: Daphane Cornelisse\n",
    "- Date: 24-Aprl-2018\n",
    "\n",
    "\n",
    "- Title: **Understanding of Convolutional Neural Network (CNN) - Deep Learning**\n",
    "- Link: https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
    "- Author: Prabhu\n",
    "- Date: 04-Mar-2018\n",
    "\n",
    "\n",
    "- Title: **Implementation of Training Convolutional Neural Networks**\n",
    "- Link: https://arxiv.org/ftp/arxiv/papers/1506/1506.01195.pdf\n",
    "- Authors: Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang\n",
    "- University of Chinese Academy of Sciences, Beijing, China\n",
    "\n",
    "\n",
    "- Title: **A Beginner's Guide to Neural Networks and Deep Learning**\n",
    "- Link: https://skymind.ai/wiki/neural-network\n",
    "- Author: AI Wiki\n",
    "\n",
    "\n",
    "- Title: **LeNet5 - A Classic CNN Architecture**\n",
    "- Link: https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
    "- Author: Muhammad Rizwan\n",
    "- Date: 30-Sept-2018\n",
    "\n",
    "\n",
    "- Ref: @RetinaNet-Intro\n",
    "- Title: **RetinaNet Introduction**\n",
    "- Link: https://arxiv.org/pdf/1708.02002.pdf\n",
    "- Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar\n",
    "- Facebook AI Research (FAIR)\n",
    "\n",
    "- Title: **COCO (Community Objects in Context) Image Dataset **\n",
    "- Link: http://cocodataset.org/#home\n",
    "\n",
    "\n",
    "- Ref: @ObjDetect\n",
    "- Title: Object Detection with Deep Learning on Aerial Imagery\n",
    "- Link: https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9\n",
    "- Author: Arthur Douillard\n",
    "- Date: 22-Jun-2018\n",
    "\n",
    "\n",
    "**GitHub Links:**\n",
    "\n",
    "- Title: **Convolutional Neural Network**\n",
    "- Link: https://github.com/mbadry1/DeepLearning.ai-Summary/tree/master/4-%20Convolutional%20Neural%20Networks\n",
    "- Author: Mahmoud Badry\n",
    "\n",
    "\n",
    "- Title: **Keras RetinaNet**\n",
    "- Link: https://github.com/fizyr/keras-retinanet\n",
    "- Author: Fizyr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
