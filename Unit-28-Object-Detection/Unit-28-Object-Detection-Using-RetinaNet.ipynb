{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unit 28 - Project: Object Detection using RetinaNet\n",
    "\n",
    "**Project**: Object Detection, Classification and Labeling using RetinaNet\n",
    "\n",
    "\n",
    "## Course: Fall 2018, Deep Learning\n",
    "Professor: **Dr. James Shanahan**\n",
    "\n",
    "Students: **Gelesh Omathil and Murali Cheruvu**\n",
    "\n",
    "University: **Indiana University**\n",
    "\n",
    "\n",
    "**RetinaNet**: Introduction: https://arxiv.org/pdf/1708.02002.pdf\n",
    "\n",
    "**Dataset**: \n",
    "-\tUse **COCO Dataset** (http://cocodataset.org/#home) (~100k images) for training, validation and test datasets \n",
    "-\tAbout 1MM bounding boxes; some of the images have about 10 classes in them\n",
    "-\tWe have 80 classes in this database\n",
    "-\tOur focus is only 7 classes - **car, truck, person, bus, bicycle and traffic sign**\n",
    "\n",
    "**Cloud**: Cloud Provider Server with Linux/Ubuntu Box with **GPU**s\n",
    "\n",
    "**Project**: **Train RetinaNet Dataset - Object Detectors**\n",
    "\n",
    "-\tUse this notebook as a base: https://github.com/fizyr/keras-retinanet\n",
    "-\tUse Transfer Learning (load the weights from pre-trained models)\n",
    "-\tUse base model on the pre-trained model from RetinaNet but focus on only 7 classes (all the other classes be treated like background images)\n",
    "-\tRetrain part of the network (about 6 key layers) from the transferred learning state using ResNet50/ResNet101 as a back-bone - with focus on 7 classes, so that we will recalibrate our model\n",
    "-\tPredict bounding boxes, predict classes - 8 classes (7 + background class), 8X5 outputs\n",
    "-\tTry out - output layers of different resolutions - ex: 56X56, 28x28, 14x14 (feature pyramid)\n",
    "-\tFor each feature pyramid, we will have output layer with loss function\n",
    "-\tTry out with smaller epochs with CPU and full blown using GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries\n",
    "\n",
    "- Python 3.6 \n",
    "- Keras 2.2.4+\n",
    "- TensorFlow (CPU and GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recognizing an object from an image has always been a very challenging task. If we need detect multiple objects from the same image, it is even more difficult. Purpose of Computer Vision is to solve such complex tasks. With the emergence of Neural Network driven Machine Learning algorithms, there are better ways to tackle these tasks. \n",
    "\n",
    "Object detection architecture is categorized into two types: two-stage and single-stage. \n",
    "\n",
    "Two-stage object detectors organize the image into two parts: foreground and background. Then, all the foreground objects are classified into more fine grained classes: car, truck, person, bus, bicycle, etc. \n",
    "\n",
    "Convolutional Neural Network (CNN), Deep Learning, is an advanced neural network concept to perfectly handle these challenges.\n",
    "\n",
    "We present three techniques here - (1) Region-based CNN (R-CNN), (2)  Fast R-CNN and (3) Regional Proposal Network (RPN) (Ref: @Guide-DL).\n",
    "\n",
    "1. **Region-based CNN**\n",
    "\n",
    "![image.png](img/r-cnn.png)(Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "2. **Fast R-CNN**\n",
    "\n",
    "![image.png](img/faster-r-cnn.png) (Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "3. **Regional Proposal Network**\n",
    "\n",
    "![](img/reg_prop_1.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/reg_prop_2.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet - One-Stage Detector\n",
    "\n",
    "Most of the popular object detector algorithms are based on R-CNN with two-stage detection and give highest possible accuracy.\n",
    "However, two-stage detection algorithms are slower due to complex processing in a iterative manner. \n",
    "\n",
    "Recent work to improve the performance of the algorithms, one-stage detectors come to popularity. **OverFeat** and **YOLO** (You Only Look Once) have achieved faster detection with 10%-40% accuracy relative to two-stage detectors. \n",
    "\n",
    "RetinaNet, Focal Loss for Dense Object Detection, is a project done by Facebook AI Research team, has proposed one-stage detector with hybrid approaches from two-stage detectors such as Feature Pyramid Network (FPN) and Mask R-CNN, to achieve the accuracy comparable with two-stage detectors. RetinaNet offers best of both single-stage and two-stage detectors.\n",
    "\n",
    "Following figure shows the comparison of various object detection algorithms including RetinaNet:\n",
    "\n",
    "![image.png](img/retinanet-compare.png) (Ref: @ObjDetect)\n",
    "\n",
    "Some of the key aspects are listed as follows:\n",
    "\n",
    "- First-pass detection, class imbalance and inefficiency is addressed using techniques such as bootstrapping and hard example mining\n",
    "- Proposed a new loss function, **Focal Loss**, dynamically scaled cross entropy loss to deal with class imbalance using intuitive scaling factor to down-weight the contribution of easy samples automatically while focusing on the hard samples\n",
    "\n",
    "\n",
    "(Ref: @RetinaNet-Intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet Components\n",
    "\n",
    "\"**RetinaNet is a single, unified network composed of a Feature Pyramid (backbone) network and two task-specific sub-networks**\" (Ref: @RetinaNet-Intro)\n",
    "\n",
    "![RetinaNet](img/retinanet.png \"RetinaNet Architecture\") (Ref: @RetinaNet-Intro)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet, CNN Network as Backbone\n",
    "\n",
    "ResNet-50 is a popular convolutional neural network for images. It processes images by going through several convolutional filters/kernels to create various feature-maps of the images to capture high level features, then it goes down into details with smaller feature maps by using pooling layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pyramid Network\n",
    "\n",
    "RetinaNet adds a Feature Pyramid Network (FPN), instead of, the typical classifier. Thus, RetinaNet collects feature maps at various layers from the ResNet and provides complex features at different scales. It is called pyramid network because it detects objects at different scales at different levels as it goes up in the pyramid. \n",
    "\n",
    "![RetinaNet](img/feature-pyramid-network.png \"Feature Pyramid Network\") (Ref: @RetinaNet-Int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Boxes\n",
    "\n",
    "An anchor is a rectangle box with different sizes and ratios. At each FPN level, anchors are created in association with feature maps, covering each potential object. \n",
    "\n",
    "Each FPN level goes through two fully convolutional networks (FCN), first one is to find the regression - predicts anchor box boundaries - x1, y1, x2, y2 and the second neural network is for multi-label (N) classification. \n",
    "\n",
    "\n",
    "![RetinaNet](img/anchor-boxes.png \"Anchor Boxes\") (Ref: @RetinaNet-Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss\n",
    "\n",
    "Real improvement in the accuracy of the RetinaNet is brought by using a new loss function called - Focal Loss.\n",
    "\n",
    "Focal Loss is designed to address the image imbalance challenge between foreground and background classes during the training of the image dataset. Focal Loss assigns low-weights to the well-defined backgrounds. \n",
    "\n",
    "Focal Loss for the binary classification, similar to Cross Entropy (CE):\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        CE_{(p,y)}=\\begin{cases}\n",
    "                -log(p) & \\text{if }y = 1\\,,  \\\\\n",
    "                -log(1 - p) & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "In the above y belongs to {+/- 1} denotes the base class (ground-truth) and p = [0,1] is the estimated probability of the model for the class with label y = 1. We define p as:\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        p_{t}=\\begin{cases}\n",
    "                p & \\text{if }y = 1\\,,  \\\\\n",
    "                1 - p & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        Fl(p_{t})= -(1 - p_{t})^{y} log(p_{t})\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the focal loss function defined above, classification cross-entropy loss -log(p) by a factor of (1-p)^y. Here is y is the modulating factor between 0 and 5. The well classified background classes have higher p and lower y. This is key aspect that compels the model to learn on specific foreground classes. \n",
    "\n",
    "\n",
    "\n",
    "![RetinaNet](img/focal-loss.png \"Focal Loss\") (Ref: @ObjDetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For complete details of the Focal Loss Object Detetion - Single-Stage Detector algorithm, please refer to the link:  https://arxiv.org/pdf/1708.02002.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- Prepare the dataset in the CSV format (with training and cross-validaton split)\n",
    "- Check the correctness of the dataset using retinanet-debug\n",
    "- Train retinanet, using predefined COCO weights (with decent jump start with better accuracy and better performance)\n",
    "- Optimize the training model to an inference model\n",
    "- Evaluate the updated model on the cross-validaton and test datasets\n",
    "- install pycocotools to test on the MS COCO dataset by running pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- COCO dataset can be trained on RetinaNet using the python code lised in the training folder\n",
    "- The default backbone is ResNet50, it can be changed to a different dataset by pasing the dataset name in the --backbone argument\n",
    "- Various backbone models to try are: ResNet models (ResNet50, ResNet101), MobileNet models (MobileNet128_1:0, MobileNet128_0.75) and VGG models\n",
    "- Probability of all the anchor boxes is set to 0.01\n",
    "- weight decay of 0.0001 and momentum of 0.9 with initial learning rate of 0.01 is used for first 60k iterations. Learning rate is reduced by 10 after 60k to 80k iterations\n",
    "- Achieved mAP = 40.8 using ResNeXt-101-FPN backbone on MS COCO dataset\n",
    "\n",
    "Trained model needs to converted into an inteference model before proceeding to the testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Usage\n",
    "\n",
    "### Running directly from the repository:\n",
    "keras_retinanet/bin/train.py coco /path/to/MS/COCO\n",
    "\n",
    "### Using the installed script:\n",
    "retinanet-train coco /path/to/MS/COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Next Steps - Projects\n",
    "\n",
    "- NATO Innovation Challenge. The winning team of the NATO Innovation Challenge used keras-retinanet to detect cars in aerial images (COWC dataset).\n",
    "\n",
    "- Microsoft Research for Horovod on Azure. A research project by Microsoft, using keras-retinanet to distribute training over multiple GPUs using Horovod on Azure.\n",
    "\n",
    "- 4k video example. This demo shows the use of keras-retinanet on a 4k input video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# References: \n",
    "\n",
    "We would like thank our professor - **Dr. James Shanahan** for his great guidance, continual help and support during the **Deep Learning course.**\n",
    "\n",
    "We would also like to thank various developers and authors of the Deep Learning (CNN) related including the references given in the following links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Books **\n",
    "\n",
    "- Ref: Book_DL\n",
    "- Book Title: **Deep Learning**\n",
    "- Authors: Ian Goodfellow, Yoshua Bengio and Aaron Courville\n",
    "\n",
    "\n",
    "- Ref: Guide-DL\n",
    "- Book/Guide: **A Guide to Covolutional Neural Networks for Computer Vision**\n",
    "- Link: https://www.dropbox.com/s/789qiaq0svh4270/A%20Guide%20to%20Convolutional%20Neural%20Networks%20for%20Computer%20Vision.pdf?dl=0\n",
    "- Editors: GÃ©rard Medioni, University of Southern California and Sven Dickinson, University of Toronto\n",
    "\n",
    "** Videos **\n",
    "\n",
    "- Title: **Courseera CNN course - Object Detection and Localization**\n",
    "- Link: https://www.coursera.org/lecture/convolutional-neural-networks/object-detection-VgyWR\n",
    "- Professor: Andrew Ng\n",
    "\n",
    "\n",
    "** Web Articles **\n",
    "\n",
    "- Title: **Back-Propogation is very simple. Who made it complicated?**\n",
    "- Link: https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "- Author: Prakash Jay\n",
    "- Date: 20-Apr-2017\n",
    "\n",
    "\n",
    "- Title: **An intutive guide to Convolutional Neural Networks**\n",
    "- Link: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\n",
    "- Author: Daphane Cornelisse\n",
    "- Date: 24-Aprl-2018\n",
    "\n",
    "\n",
    "- Title: **Understanding of Convolutional Neural Network (CNN) - Deep Learning**\n",
    "- Link: https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
    "- Author: Prabhu\n",
    "- Date: 04-Mar-2018\n",
    "\n",
    "\n",
    "- Title: **Implementation of Training Convolutional Neural Networks**\n",
    "- Link: https://arxiv.org/ftp/arxiv/papers/1506/1506.01195.pdf\n",
    "- Authors: Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang\n",
    "- University of Chinese Academy of Sciences, Beijing, China\n",
    "\n",
    "\n",
    "- Title: **A Beginner's Guide to Neural Networks and Deep Learning**\n",
    "- Link: https://skymind.ai/wiki/neural-network\n",
    "- Author: AI Wiki\n",
    "\n",
    "\n",
    "- Title: **LeNet5 - A Classic CNN Architecture**\n",
    "- Link: https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
    "- Author: Muhammad Rizwan\n",
    "- Date: 30-Sept-2018\n",
    "\n",
    "\n",
    "- Ref: @RetinaNet-Intro\n",
    "- Title: **RetinaNet Introduction**\n",
    "- Link: https://arxiv.org/pdf/1708.02002.pdf\n",
    "- Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar\n",
    "- Facebook AI Research (FAIR)\n",
    "\n",
    "- Title: **COCO (Community Objects in Context) Image Dataset **\n",
    "- Link: http://cocodataset.org/#home\n",
    "\n",
    "\n",
    "- Ref: @ObjDetect\n",
    "- Title: Object Detection with Deep Learning on Aerial Imagery\n",
    "- Link: https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9\n",
    "- Author: Arthur Douillard\n",
    "- Date: 22-Jun-2018\n",
    "\n",
    "- Ref: @RetinaNet-Int\n",
    "- Title: The intuition behind RetinaNet\n",
    "- Link: https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d\n",
    "- Author: Prakash Jay\n",
    "- Date: 23-Mar-2018\n",
    "\n",
    "**GitHub Links:**\n",
    "\n",
    "- Title: **Convolutional Neural Network**\n",
    "- Link: https://github.com/mbadry1/DeepLearning.ai-Summary/tree/master/4-%20Convolutional%20Neural%20Networks\n",
    "- Author: Mahmoud Badry\n",
    "\n",
    "\n",
    "- Title: **Keras RetinaNet**\n",
    "- Link: https://github.com/fizyr/keras-retinanet\n",
    "- Author: Fizyr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
