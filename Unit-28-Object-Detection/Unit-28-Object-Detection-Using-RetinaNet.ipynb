{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Unit 28 - Project: Object Detection using RetinaNet\n",
    "\n",
    "**Project**: Object Detection, Classification and Labeling using RetinaNet\n",
    "\n",
    "\n",
    "## Course: Fall 2018, Deep Learning\n",
    "Professor: **Dr. James Shanahan**\n",
    "\n",
    "Students: **Gelesh Omathil and Murali Cheruvu**\n",
    "\n",
    "University: **Indiana University**\n",
    "\n",
    "\n",
    "**RetinaNet**: Introduction: https://arxiv.org/pdf/1708.02002.pdf\n",
    "\n",
    "**Dataset**: \n",
    "-\tUse **COCO Dataset** (http://cocodataset.org/#home) (~100k images) for training, validation and test datasets \n",
    "-\tAbout 1MM bounding boxes; some of the images have about 10 classes in them\n",
    "-\tWe have 80 classes in this database\n",
    "-\tOur focus is only 7 classes - **car, truck, person, bus, bycle and traffic sign**\n",
    "\n",
    "**Cloud**: Cloud Provider Server with Linux/Ubuntu Box with **GPU**s\n",
    "\n",
    "**Project**: **Train RetinaNet Dataset - Object Detectors**\n",
    "\n",
    "-\tUse this notebook as a base: https://github.com/fizyr/keras-retinanet\n",
    "-\tUse Transfer Learning (load the weights from pre-trained models)\n",
    "-\tUse base model on the pre-trained model from RetinaNet but focus on only 7 classes (all the other classes be treated like background images)\n",
    "-\tRetrain part of the network (about 6 key layers) from the transferred learning state using ResNet50/ResNet101 as a back-bone - with focus on 7 classes, so that we will recalibrate our model\n",
    "-\tPredict bounding boxes, predict classes - 8 classes (7 + background class), 8X5 outputs\n",
    "-\tTry out - output layers of different resolutions - ex: 56X56, 28x28, 14x14 (feature pyramid)\n",
    "-\tFor each feature pyramid, we will have output layer with loss function\n",
    "-\tTry out with smaller epochs with CPU and full blown using GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries\n",
    "\n",
    "- Python 3.6 \n",
    "- Keras 2.2.4+\n",
    "- TensorFlow (CPU and GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Recognizing an object from an image has always been a very challenging task. If we need detect multiple objects from the same image is even more difficult. Purpose of Computer Vision is to solve such complex tasks. With the emergence of Neural Network driven Machine Learning algorithms, there are better ways to tackle these tasks. \n",
    "\n",
    "Convolutional Neural Network (CNN), Deep Learning, is an advanced neural network concept to perfectly handle these challenges.\n",
    "\n",
    "We present three techniques here - (1) Region-based CNN (R-CNN), (2)  Fast R-CNN and (3) Regional Proposal Network (RPN) (Ref: @Guide-DL).\n",
    "\n",
    "1. **Region-based CNN**\n",
    "\n",
    "![image.png](img/r-cnn.png)(Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "2. **Fast R-CNN**\n",
    "\n",
    "![image.png](img/faster-r-cnn.png) (Ref: @Guide-DL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "3. **Regional Proposal Network**\n",
    "\n",
    "![](img/reg_prop_1.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "![image.png](img/reg_prop_2.png) (Ref: @Guide-DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## RetinaNet - One-Stage Detector\n",
    "\n",
    "Most of the popular object detector algorithms are based on R-CNN with two-stage detection and give highest possible accuracy.\n",
    "However, two-stage detection algorithms are slower due to complex processing in a iterative manner. Recent work to improve the performance of the algorithms, one-stage detectors come to popularity. **OverFeat** and **YOLO** (You Only Look Once) have achieved faster detection with 10%-40% accuracy relative to two-stage detectors. \n",
    "\n",
    "Focal Loss for Dense Object Detection, is a project done by Facebook AI Reserch team, has proposed one-stage detector with hybrid approaches from two-stage detectors such as Feature Pyramid Network (FPN) and Mask R-CNN, to achieve the acurracy comparable with two-stage detectors.\n",
    "\n",
    "Some of the key aspects are listed as follows:\n",
    "\n",
    "- First-pass detection, class imbalance and inefficiency is addressed using techniques such as bootstrapping and hard example mining\n",
    "- Proposed a new loss function, **Focal Loss**, dynamically scalled cross entropy loss to deal with class imbalance using intutive scaling factor to down-weight the contribution of easy samples automatically while focusing on the hard samples\n",
    "\n",
    "\n",
    "(Ref: @RetinaNet-Intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Focal Loss\n",
    "\n",
    "Focal Loss is desinged to address the image imbalance challege between foreground and background classes during the training of the image dataset. \n",
    "\n",
    "Focal Loss for the binary classfication, similar to Cross Entropy (CE):\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        CE_{(p,y)}=\\begin{cases}\n",
    "                -log(p) & \\text{if }y = 1\\,,  \\\\\n",
    "                -log(1 - p) & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n",
    "\n",
    "In the above y belongs to {+/- 1} denotes the base class (ground-truth) and p = [0,1] is the estimated probability of the model for the class with label y = 1. We deine p as:\n",
    "\n",
    "\\begin{equation*}\n",
    "[\n",
    "        p_{t}=\\begin{cases}\n",
    "                p & \\text{if }y = 1\\,,  \\\\\n",
    "                1 - p & \\text{if } otherwise\\,.\n",
    "        \\end{cases}\n",
    "]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**RetinaNet is a single, unified network composed of a Feature Pyramid (backbone) network and two task-specific sub-networks**\" (Ref: @RetinaNet-Intro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "![RetinaNet](img/retinanet.png \"Title\") (Ref: @RetinaNet-Intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For complete details of the Focal Loss Object Detetion - Single-Stage Detector algorithm, please refer to the link:  https://arxiv.org/pdf/1708.02002.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- Prepare the dataset in the CSV format (with training and cross-validaton split)\n",
    "- Check the correctness of the dataset using retinanet-debug\n",
    "- Train retinanet, using predefined COCO weights (with decent jump start with better accuracy and better performance)\n",
    "- Optimize the training model to an inference model\n",
    "- Evaluate the updated model on the cross-validaton and test datasets\n",
    "- install pycocotools to test on the MS COCO dataset by running pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "- COCO dataset can be trained on RetinaNet using the python code lised in the training folder\n",
    "- The default backbone is ResNet50, it can be changed to a different dataset by pasing the dataset name in the --backbone argument\n",
    "- Various backbone models to try are: ResNet models (ResNet50, ResNet101), MobileNet models (MobileNet128_1:0, MobileNet128_0.75) and VGG models\n",
    "\n",
    "Trained model needs to converted into an inteference model before proceeding to the testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Usage\n",
    "\n",
    "### Running directly from the repository:\n",
    "keras_retinanet/bin/train.py coco /path/to/MS/COCO\n",
    "\n",
    "### Using the installed script:\n",
    "retinanet-train coco /path/to/MS/COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RetinaNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this to point to your downloaded/trained model\n",
    "# models can be downloaded here: https://github.com/fizyr/keras-retinanet/releases\n",
    "model_path = os.path.join('..', 'snapshots', 'resnet50_coco_best_v2.1.0.h5')\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# if the model is not converted to an inference model, use the line below\n",
    "# see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model\n",
    "#model = models.convert_model(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'person',  1: 'bicycle',  2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', \n",
    "                   6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', \n",
    "                   11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', \n",
    "                   16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', \n",
    "                   21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', \n",
    "                   26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', \n",
    "                   31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', \n",
    "                   36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', \n",
    "                   41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', \n",
    "                   46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', \n",
    "                   51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', \n",
    "                   56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', \n",
    "                   61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', \n",
    "                   66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', \n",
    "                   71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', \n",
    "                   76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_img_retinaNet(img_name):\n",
    "    # load image\n",
    "    image = read_image_bgr(img_name)\n",
    "\n",
    "    # copy to draw on\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    print(\"processing time: \", time.time() - start)\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < 0.5:\n",
    "            break\n",
    "\n",
    "        color = label_color(label)\n",
    "\n",
    "        b = box.astype(int)\n",
    "        draw_box(draw, b, color=color)\n",
    "\n",
    "        caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "        draw_caption(draw, b, caption)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#car, truck, person, bus, bycle and traffic sign\n",
    "classify_img_retinaNet('img_with_car.jpg')\n",
    "classify_img_retinaNet('img_with_truck.jpg')\n",
    "classify_img_retinaNet('img_with_person.jpg')\n",
    "classify_img_retinaNet('img_with_bus.jpg')\n",
    "classify_img_retinaNet('img_with_bycle.jpg')\n",
    "classify_img_retinaNet('img_with_traffic_sign.jpg')\n",
    "classify_img_retinaNet('img_with_multiple.jpg')\n",
    "classify_img_retinaNet('img_with_all.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Next Steps - Projects\n",
    "\n",
    "- NATO Innovation Challenge. The winning team of the NATO Innovation Challenge used keras-retinanet to detect cars in aerial images (COWC dataset).\n",
    "\n",
    "- Microsoft Research for Horovod on Azure. A research project by Microsoft, using keras-retinanet to distribute training over multiple GPUs using Horovod on Azure.\n",
    "\n",
    "- 4k video example. This demo shows the use of keras-retinanet on a 4k input video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# References: \n",
    "\n",
    "We would like thank our professor - **Dr. James Shanahan** for his great guidance, continual help and support during the **Deep Learning course.**\n",
    "\n",
    "We would also like to thank various developers and authors of the Deep Learning (CNN) related including the references given in the following links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "** Books **\n",
    "\n",
    "- Ref: Book_DL\n",
    "- Book Title: **Deep Learning**\n",
    "- Authors: Ian Goodfellow, Yoshua Bengio and Aaron Courville\n",
    "\n",
    "\n",
    "- Ref: Guide-DL\n",
    "- Book/Guide: **A Guide to Covolutional Neural Networks for Computer Vision**\n",
    "- Link: https://www.dropbox.com/s/789qiaq0svh4270/A%20Guide%20to%20Convolutional%20Neural%20Networks%20for%20Computer%20Vision.pdf?dl=0\n",
    "- Editors: Gérard Medioni, University of Southern California and Sven Dickinson, University of Toronto\n",
    "\n",
    "** Videos **\n",
    "\n",
    "- Title: **Courseera CNN course - Object Detection and Localization**\n",
    "- Link: https://www.coursera.org/lecture/convolutional-neural-networks/object-detection-VgyWR\n",
    "- Professor: Andrew Ng\n",
    "\n",
    "\n",
    "** Web Articles **\n",
    "\n",
    "- Title: **Back-Propogation is very simple. Who made it complicated?**\n",
    "- Link: https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "- Author: Prakash Jay\n",
    "- Date: 20-Apr-2017\n",
    "\n",
    "\n",
    "- Title: **An intutive guide to Convolutional Neural Networks**\n",
    "- Link: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\n",
    "- Author: Daphane Cornelisse\n",
    "- Date: 24-Aprl-2018\n",
    "\n",
    "\n",
    "- Title: **Understanding of Convolutional Neural Network (CNN) - Deep Learning**\n",
    "- Link: https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
    "- Author: Prabhu\n",
    "- Date: 04-Mar-2018\n",
    "\n",
    "\n",
    "- Title: **Implementation of Training Convolutional Neural Networks**\n",
    "- Link: https://arxiv.org/ftp/arxiv/papers/1506/1506.01195.pdf\n",
    "- Authors: Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang\n",
    "- University of Chinese Academy of Sciences, Beijing, China\n",
    "\n",
    "\n",
    "- Title: **A Beginner's Guide to Neural Networks and Deep Learning**\n",
    "- Link: https://skymind.ai/wiki/neural-network\n",
    "- Author: AI Wiki\n",
    "\n",
    "\n",
    "- Title: **LeNet5 - A Classic CNN Architecture**\n",
    "- Link: https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
    "- Author: Muhammad Rizwan\n",
    "- Date: 30-Sept-2018\n",
    "\n",
    "- Ref: @RetinaNet-Intro\n",
    "- Title: **RetinaNet Introduction**\n",
    "- Link: https://arxiv.org/pdf/1708.02002.pdf\n",
    "- Authors: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar\n",
    "- Facebook AI Research (FAIR)\n",
    "\n",
    "- Title: **COCO (Community Objects in Context) Image Dataset **\n",
    "- Link: http://cocodataset.org/#home\n",
    "\n",
    "\n",
    "**GitHub Links:**\n",
    "\n",
    "- Title: **Convolutional Neural Network**\n",
    "- Link: https://github.com/mbadry1/DeepLearning.ai-Summary/tree/master/4-%20Convolutional%20Neural%20Networks\n",
    "- Author: Mahmoud Badry\n",
    "\n",
    "\n",
    "- Title: **Keras RetinaNet**\n",
    "- Link: https://github.com/fizyr/keras-retinanet\n",
    "- Author: Fizyr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
